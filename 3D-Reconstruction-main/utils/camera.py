# Camera pose manipulation and trajectory generation.
import os
import torch
import numpy as np
import math
from typing import Dict, List, Optional

from scipy.spatial.transform import Slerp
from scipy.spatial.transform import Rotation as R


def interpolate_poses(key_poses: torch.Tensor, target_frames: int) -> torch.Tensor:
    """
    Interpolate between key poses to generate a smooth trajectory.

    Args:
        key_poses (torch.Tensor): Tensor of shape (N, 4, 4) containing key camera poses.
        target_frames (int): Number of frames to interpolate.

    Returns:
        torch.Tensor: Interpolated poses of shape (target_frames, 4, 4).
    """
    device = key_poses.device
    key_poses = key_poses.cpu().numpy()

    # Separate translation and rotation
    translations = key_poses[:, :3, 3]
    rotations = key_poses[:, :3, :3]

    # Create time array
    times = np.linspace(0, 1, len(key_poses))
    target_times = np.linspace(0, 1, target_frames)

    # Interpolate translations
    interp_translations = np.stack(
        [np.interp(target_times, times, translations[:, i]) for i in range(3)], axis=-1
    )

    # Interpolate rotations using Slerp
    key_rots = R.from_matrix(rotations)
    slerp = Slerp(times, key_rots)
    interp_rotations = slerp(target_times).as_matrix()

    # Combine interpolated translations and rotations
    interp_poses = np.eye(4)[None].repeat(target_frames, axis=0)
    interp_poses[:, :3, :3] = interp_rotations
    interp_poses[:, :3, 3] = interp_translations

    return torch.tensor(interp_poses, dtype=torch.float32, device=device)


def look_at_rotation(
    direction: torch.Tensor, up: torch.Tensor = torch.tensor([0.0, 0.0, 1.0])
) -> torch.Tensor:
    """Calculate rotation matrix to look at a specific direction."""
    # ç¡®ä¿è¾“å…¥å¼ é‡åœ¨ç›¸åŒè®¾å¤‡
    up = up.to(direction.device)  # [!code ++]
    front = torch.nn.functional.normalize(direction, dim=-1)
    right = torch.nn.functional.normalize(torch.cross(front, up), dim=-1)
    up = torch.cross(right, front)
    rotation_matrix = torch.stack([right, up, -front], dim=-1)
    return rotation_matrix


def get_interp_novel_trajectories(
    dataset_type: str,
    scene_idx: str,
    per_cam_poses: Dict[int, torch.Tensor],
    traj_type: str = "front_center_interp",
    target_frames: int = 100,
) -> torch.Tensor:
    original_frames = per_cam_poses[list(per_cam_poses.keys())[0]].shape[0]

    trajectory_generators = {
        "front_center_interp": front_center_interp,
        "s_curve": s_curve,
        "three_key_poses": three_key_poses_trajectory,
        # æ–°å¢è½¨è¿¹ç±»å‹
        "circle_trajectory": circle_trajectory,
        "spiral_trajectory": spiral_trajectory,
        "look_around_trajectory": look_around_trajectory,
        "fixed_path_trajectory": kitti_fixed_path,
        "analyze_center_trajectory":analyze_front_center_interp,
        "analyze_npz_trajectory":analyze_kitti_trajectory,
        "fixed_offset_1": fixed_offset_trajectory_1,
        "fixed_offset_2": fixed_offset_trajectory_2,
        "fixed_offset_3": fixed_offset_trajectory_3,
        "fixed_offset_4": fixed_offset_trajectory_4,
        "fixed_offset_5": fixed_offset_trajectory_5,
        "fixed_offset_6": fixed_offset_trajectory_6,
        "fixed_offset_7": fixed_offset_trajectory_7,
        "fixed_offset_8": fixed_offset_trajectory_8,
        "fixed_offset_9": fixed_offset_trajectory_9,
        "fixed_offset_10": fixed_offset_trajectory_10,
        "fixed_offset": fixed_offset_trajectory,
        "lane_change": smooth_lane_change_trajectory,
        "double_lane_change": double_lane_change_trajectory,
    }

    if traj_type not in trajectory_generators:
        raise ValueError(f"Unknown trajectory type: {traj_type}")

    return trajectory_generators[traj_type](
        dataset_type, per_cam_poses, original_frames, target_frames
    )

def kitti_fixed_path(
    dataset_type: str,
    per_cam_poses: Dict[int, torch.Tensor],
    original_frames: int,
    target_frames: int,
    num_loops: int = 1,
    npz_path = "output/Kitti/dataset=Kitti/change_line_gt/camera_poses_eval/full_poses_2025-07-02_18-00-20.npz",
    position_offset: Optional[List[float]] = None,  # æ–°å¢ï¼šä½ç½®åç§» [x, y, z]
    rotation_offset: Optional[List[float]] = None,  # æ–°å¢ï¼šæ—‹è½¬åç§» [roll, pitch, yaw] (å¼§åº¦)
) -> torch.Tensor:
    """
    ä»NPZæ–‡ä»¶è¯»å–å®Œæ•´çš„ç›¸æœºè½¨è¿¹ï¼Œä¸åšæ’å€¼ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®
    
    Args:
        dataset_type (str): æ•°æ®é›†ç±»å‹ï¼ˆæ­¤å‡½æ•°ä¸­æœªä½¿ç”¨ï¼‰
        per_cam_poses (Dict[int, torch.Tensor]): æ¯ç›¸æœºposesï¼ˆæ­¤å‡½æ•°ä¸­æœªä½¿ç”¨ï¼‰
        original_frames (int): åŸå§‹å¸§æ•°ï¼ˆæ­¤å‡½æ•°ä¸­æœªä½¿ç”¨ï¼‰
        target_frames (int): ç›®æ ‡å¸§æ•°ï¼ˆå¦‚æœè¶…è¿‡åŸå§‹å¸§æ•°åˆ™é‡å¤æˆ–æˆªæ–­ï¼‰
        num_loops (int): å¾ªç¯æ¬¡æ•°ï¼ˆæ­¤å‡½æ•°ä¸­æœªä½¿ç”¨ï¼‰
        position_offset (List[float], optional): ä½ç½®åç§» [x, y, z]ï¼Œå•ä½ç±³
        rotation_offset (List[float], optional): æ—‹è½¬åç§» [roll, pitch, yaw]ï¼Œå•ä½å¼§åº¦
        
    Returns:
        torch.Tensor: åŸå§‹è½¨è¿¹æ•°æ®ï¼Œå½¢çŠ¶ä¸º (actual_frames, 4, 4)
    """
    # å†™æ­»çš„NPZæ–‡ä»¶è·¯å¾„
    # npz_path = "output/Kitti/dataset=Kitti/change_line_gt/camera_poses_eval/full_poses_2025-07-02_18-00-20.npz"
    
    print(f"ğŸ” Loading complete trajectory from NPZ (no interpolation):")

    # position_offset = [0, 0, 0]
    
    try:
        # åŠ è½½NPZæ–‡ä»¶
        data = np.load(npz_path, allow_pickle=True)
        camera_poses = data['camera_poses']  # å½¢çŠ¶: (N, 4, 4)
        cam_names = data['cam_names']        # ç›¸æœºåç§°åˆ—è¡¨
        frame_indices = data['frame_indices'] # å¸§ç´¢å¼•
        
        print(f"   NPZ contains {len(camera_poses)} total poses")
        print(f"   Available cameras: {set(cam_names)}")
        
        # å¯»æ‰¾å‰è§†ä¸­å¿ƒç›¸æœºï¼ˆå°è¯•å¤šç§å¯èƒ½çš„å‘½åï¼‰
        front_center_mask = None
        found_camera = None
        
        for candidate in ['CAM_LEFT', 'FRONT_CENTER', 'front_center', 'FRONT', 'front', '0', 'cam0']:
            mask = np.array([str(name) == candidate for name in cam_names])
            if mask.any():
                front_center_mask = mask
                found_camera = candidate
                break
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªç›¸æœº
        if front_center_mask is None:
            front_center_mask = np.ones(len(cam_names), dtype=bool)
            front_center_mask[1:] = False  # åªä¿ç•™ç¬¬ä¸€ä¸ª
            found_camera = str(cam_names[0])
        
        print(f"   Using camera: {found_camera}")
        
        # æå–å‰è§†ä¸­å¿ƒç›¸æœºçš„poses
        front_center_poses = camera_poses[front_center_mask]
        front_center_frames = np.array(frame_indices)[front_center_mask]
        
        print(f"   Found {len(front_center_poses)} poses for this camera")
        
        # æŒ‰å¸§ç´¢å¼•æ’åº
        sorted_indices = np.argsort(front_center_frames)
        front_center_poses = front_center_poses[sorted_indices]
        front_center_frames = front_center_frames[sorted_indices]
        
        print(f"   Frame range: {front_center_frames[0]} - {front_center_frames[-1]}")
        
        # æ˜¾ç¤ºä½ç½®èŒƒå›´
        positions = front_center_poses[:, :3, 3]
        print(f"   Position ranges:")
        print(f"     X: [{positions[:, 0].min():.6f}, {positions[:, 0].max():.6f}] m")
        print(f"     Y: [{positions[:, 1].min():.6f}, {positions[:, 1].max():.6f}] m")
        print(f"     Z: [{positions[:, 2].min():.6f}, {positions[:, 2].max():.6f}] m")
        
        # è½¬æ¢ä¸ºtorch tensor
        poses_tensor = torch.tensor(front_center_poses, dtype=torch.float32)
        
        # ç¡®ä¿è®¾å¤‡ä¸€è‡´æ€§
        if per_cam_poses and len(per_cam_poses) > 0:
            sample_pose = per_cam_poses[list(per_cam_poses.keys())[0]]
            poses_tensor = poses_tensor.to(sample_pose.device)
        
        # æ ¹æ®target_framesè°ƒæ•´è¾“å‡º
        actual_frames = len(poses_tensor)
        
        if target_frames <= actual_frames:
            # å¦‚æœç›®æ ‡å¸§æ•°å°‘äºæˆ–ç­‰äºå®é™…å¸§æ•°ï¼Œç›´æ¥æˆªå–
            result = poses_tensor[:target_frames]
            print(f"   Truncated to {target_frames} frames (from {actual_frames})")
        else:
            # å¦‚æœç›®æ ‡å¸§æ•°å¤šäºå®é™…å¸§æ•°ï¼Œé‡å¤æœ€åä¸€å¸§
            result = torch.zeros(target_frames, 4, 4, dtype=poses_tensor.dtype, device=poses_tensor.device)
            result[:actual_frames] = poses_tensor
            # ç”¨æœ€åä¸€å¸§å¡«å……å‰©ä½™éƒ¨åˆ†
            for i in range(actual_frames, target_frames):
                result[i] = poses_tensor[-1]
            print(f"   Extended to {target_frames} frames (repeated last frame)")
        
        # ==================== æ–°å¢ï¼šåº”ç”¨åç§» ====================
        if position_offset is not None or rotation_offset is not None:
            print(f"   Applying offsets...")
            result = apply_trajectory_offset(result, position_offset, rotation_offset)
        
        # è¾“å‡ºç»“æœä¿¡æ¯
        result_positions = result[:, :3, 3]
        print(f"   Output: {result.shape[0]} frames")
        print(f"   Start: {result_positions[0][0]:.3f}, {result_positions[0][1]:.3f}, {result_positions[0][2]:.3f}")
        print(f"   End:   {result_positions[-1][0]:.3f}, {result_positions[-1][1]:.3f}, {result_positions[-1][2]:.3f}")
        
        return result
        
    except Exception as e:
        print(f"Error loading from NPZ: {e}")
        # å¦‚æœNPZåŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹çš„front_center_interp
        print("Falling back to front_center_interp")
        assert 0 in per_cam_poses.keys(), "Front center camera (ID 0) is required for fallback"
        key_poses = per_cam_poses[0][::original_frames // 4]
        return interpolate_poses(key_poses, target_frames)


def apply_trajectory_offset(
    poses: torch.Tensor, 
    position_offset: Optional[List[float]] = None,
    rotation_offset: Optional[List[float]] = None
) -> torch.Tensor:
    """
    ç»™è½¨è¿¹åº”ç”¨ä½ç½®å’Œæ—‹è½¬åç§»
    
    Args:
        poses: åŸå§‹poseså¼ é‡ï¼Œå½¢çŠ¶ (N, 4, 4)
        position_offset: ä½ç½®åç§» [x, y, z]ï¼Œå•ä½ç±³
        rotation_offset: æ—‹è½¬åç§» [roll, pitch, yaw]ï¼Œå•ä½å¼§åº¦
        
    Returns:
        torch.Tensor: åº”ç”¨åç§»åçš„poses
    """
    import torch
    import math
    
    result = poses.clone()
    
    # åº”ç”¨ä½ç½®åç§»
    if position_offset is not None:
        offset_tensor = torch.tensor(position_offset, dtype=poses.dtype, device=poses.device)
        print(f"     Position offset: {position_offset}")
        
        # æ–¹æ³•1ï¼šç®€å•çš„å…¨å±€åç§»ï¼ˆåœ¨ä¸–ç•Œåæ ‡ç³»ä¸­ï¼‰
        result[:, :3, 3] += offset_tensor
        
        # æ–¹æ³•2ï¼šç›¸å¯¹äºç›¸æœºæœå‘çš„åç§»ï¼ˆå¦‚æœä½ æƒ³è¦ç›¸å¯¹åç§»ï¼Œå¯ä»¥å¯ç”¨è¿™ä¸ªï¼‰
        # for i in range(len(result)):
        #     # è·å–å½“å‰ç›¸æœºçš„æ—‹è½¬çŸ©é˜µ
        #     rotation_matrix = result[i, :3, :3]
        #     # å°†åç§»è½¬æ¢åˆ°ç›¸æœºåæ ‡ç³»
        #     relative_offset = rotation_matrix @ offset_tensor
        #     result[i, :3, 3] += relative_offset
    
    # åº”ç”¨æ—‹è½¬åç§»
    if rotation_offset is not None:
        print(f"     Rotation offset (roll, pitch, yaw): {rotation_offset}")
        
        # å°†æ¬§æ‹‰è§’è½¬æ¢ä¸ºæ—‹è½¬çŸ©é˜µ
        roll, pitch, yaw = rotation_offset
        
        # åˆ›å»ºæ—‹è½¬çŸ©é˜µï¼ˆZYXé¡ºåºï¼‰
        cos_r, sin_r = math.cos(roll), math.sin(roll)
        cos_p, sin_p = math.cos(pitch), math.sin(pitch) 
        cos_y, sin_y = math.cos(yaw), math.sin(yaw)
        
        # Roll (Xè½´æ—‹è½¬)
        R_x = torch.tensor([
            [1, 0, 0],
            [0, cos_r, -sin_r],
            [0, sin_r, cos_r]
        ], dtype=poses.dtype, device=poses.device)
        
        # Pitch (Yè½´æ—‹è½¬)
        R_y = torch.tensor([
            [cos_p, 0, sin_p],
            [0, 1, 0],
            [-sin_p, 0, cos_p]
        ], dtype=poses.dtype, device=poses.device)
        
        # Yaw (Zè½´æ—‹è½¬)
        R_z = torch.tensor([
            [cos_y, -sin_y, 0],
            [sin_y, cos_y, 0],
            [0, 0, 1]
        ], dtype=poses.dtype, device=poses.device)
        
        # ç»„åˆæ—‹è½¬çŸ©é˜µ (ZYXé¡ºåº)
        R_offset = R_z @ R_y @ R_x
        
        # åº”ç”¨æ—‹è½¬åç§»åˆ°æ¯ä¸€å¸§
        for i in range(len(result)):
            # åŸå§‹æ—‹è½¬çŸ©é˜µ
            original_rotation = result[i, :3, :3]
            # åº”ç”¨åç§»æ—‹è½¬
            result[i, :3, :3] = R_offset @ original_rotation
    
    return result
    
def front_center_interp(
    dataset_type: str,
    per_cam_poses: Dict[int, torch.Tensor],
    original_frames: int,
    target_frames: int,
    num_loops: int = 1,
) -> torch.Tensor:
    """Interpolate key frames from the front center camera."""
    assert (
        0 in per_cam_poses.keys()
    ), "Front center camera (ID 0) is required for front_center_interp"
    key_poses = per_cam_poses[0][
        :: original_frames // 4
    ]  # Select every 4th frame as key frame
    return interpolate_poses(key_poses, target_frames)


def fixed_offset_trajectory(
    dataset_type: str,
    per_cam_poses: Dict[int, torch.Tensor],
    original_frames: int,
    target_frames: int,
    translation_offset: list = [-4.0, 0.0, 0.0],
    rotation_offset: list = [0.0, 0.0, 0.0],
) -> torch.Tensor:
    """
    ç”Ÿæˆç›¸å¯¹äºå‰è§†ç›¸æœºçš„å›ºå®šåç§»è½¨è¿¹

    Args:
        translation_offset (list): [x, y, z] å¹³ç§»åç§»é‡ï¼ˆç±³ï¼‰
        rotation_offset (list): [pitch, yaw, roll] æ—‹è½¬åç§»é‡ï¼ˆåº¦ï¼‰
    """
    assert 0 in per_cam_poses.keys(), "éœ€è¦å‰è§†ä¸­å¿ƒç›¸æœºï¼ˆID 0ï¼‰"

    # è·å–è®¾å¤‡ä¿¡æ¯
    device = per_cam_poses[0].device

    # è½¬æ¢åç§»é‡ä¸ºå¼ é‡
    trans_offset = torch.tensor(translation_offset, device=device, dtype=torch.float32)
    rot_offset = torch.tensor(rotation_offset, device=device, dtype=torch.float32)

    # ç¡®ä¿original_framesè‡³å°‘ä¸º1
    original_frames = max(1, original_frames)
    # è®¡ç®—æ­¥é•¿ï¼Œç¡®ä¿è‡³å°‘ä¸º1
    step = max(1, original_frames // 4)
    key_poses = per_cam_poses[0][::step]

    def convert_to_tensor(data, device):
        return torch.tensor(data, device=device, dtype=torch.float32)  # [!code ++]

    # åº”ç”¨åç§»é‡
    modified_poses = []
    for pose in key_poses:
        # åˆ›å»ºæ–°ä½å§¿çŸ©é˜µ
        new_pose = torch.eye(4, device=device)

        rot_matrix = R.from_euler(
            "xyz", rot_offset.cpu().numpy(), degrees=True
        ).as_matrix()
        rot_matrix = rot_matrix.astype(np.float32)  # [!code ++]

        # ä¿®æ”¹ç‚¹3ï¼šä¿æŒçŸ©é˜µä¹˜æ³•æ•°æ®ç±»å‹ä¸€è‡´
        new_rot = pose[:3, :3] @ convert_to_tensor(rot_matrix, device)  # [!code ++]

        # ä¿®æ”¹ç‚¹4ï¼šç¡®ä¿å¹³ç§»åç§»é‡æ•°æ®ç±»å‹æ­£ç¡®
        trans_offset = convert_to_tensor(translation_offset, device)  # [!code ++]
        offset_trans = pose[:3, :3] @ trans_offset
        new_trans = pose[:3, 3] + offset_trans

        new_pose[:3, :3] = new_rot
        new_pose[:3, 3] = new_trans

        modified_poses.append(new_pose)
    # ç¡®ä¿è‡³å°‘æœ‰ä¸¤ä¸ªä½å§¿æ‰èƒ½æ’å€¼
    if len(modified_poses) == 1:
        # å¦‚æœåªæœ‰ä¸€ä¸ªä½å§¿ï¼Œç›´æ¥å¤åˆ¶å®ƒæ¥åˆ›å»ºç›®æ ‡å¸§æ•°
        return modified_poses[0].unsqueeze(0).repeat(target_frames, 1, 1)
    return interpolate_poses(torch.stack(modified_poses), target_frames)


def analyze_front_center_interp(
    dataset_type: str,
    per_cam_poses: Dict[int, torch.Tensor],
    original_frames: int,
    target_frames: int,
    num_loops: int = 1,
) -> torch.Tensor:
    """
    åˆ†æfront_center_interpçš„é€»è¾‘ï¼Œæ˜¾ç¤ºå…³é”®ä¿¡æ¯
    """
    print(f"ğŸ” Front Center Interp Analysis:")
    
    # æ£€æŸ¥è¾“å…¥
    assert 0 in per_cam_poses.keys(), "Front center camera (ID 0) required"
    front_poses = per_cam_poses[0]
    
    # åŸºæœ¬ä¿¡æ¯
    print(f"   Input: {len(front_poses)} poses -> Target: {target_frames} frames")
    print(f"   Original frames param: {original_frames}")
    
    # å…³é”®å¸§é€‰æ‹©é€»è¾‘
    step = original_frames // 4
    key_poses = front_poses[::step]
    print(f"   Step size: {step} -> Key frames: {len(key_poses)}")
    
    # æ˜¾ç¤ºå…³é”®å¸§åæ ‡
    print(f"   Key frame positions:")
    for i, pose in enumerate(key_poses):
        pos = pose[:3, 3]
        print(f"     [{i}] {pos[0]:.3f}, {pos[1]:.3f}, {pos[2]:.3f}")
    
    # è¿›è¡Œæ’å€¼
    result = interpolate_poses(key_poses, target_frames)
    
    # è¾“å‡ºç»“æœ
    result_start = result[0][:3, 3]
    result_end = result[-1][:3, 3]
    print(f"   Output: {result.shape[0]} frames")
    print(f"   Start: {result_start[0]:.3f}, {result_start[1]:.3f}, {result_start[2]:.3f}")
    print(f"   End:   {result_end[0]:.3f}, {result_end[1]:.3f}, {result_end[2]:.3f}")
    
    return result


def analyze_kitti_trajectory(
    dataset_type: str,
    per_cam_poses: Dict[int, torch.Tensor],
    original_frames: int,
    target_frames: int,
    num_loops: int = 1,
) -> torch.Tensor:
    """
    åˆ†ækittiè½¨è¿¹çš„é€»è¾‘
    """
    npz_path =  "output/Kitti/dataset=Kitti/training_20250630_162211_FollowLeadingVehicleWithObstacle_1/camera_poses_eval/full_poses_2025-07-02_18-00-20.npz"
    
    print(f"ğŸ” Kitti Trajectory Analysis:")
    
    try:
        # åŠ è½½æ•°æ®
        data = np.load(npz_path, allow_pickle=True)
        camera_poses = data['camera_poses']
        cam_names = data['cam_names']
        
        # æ‰¾åˆ°å‰è§†ä¸­å¿ƒç›¸æœº
        front_mask = None
        for candidate in ['FRONT_CENTER', 'front_center', 'FRONT', 'front', '0']:
            mask = np.array([str(name) == candidate for name in cam_names])
            if mask.any():
                front_mask = mask
                break
        
        if front_mask is None:
            front_mask = np.ones(len(cam_names), dtype=bool)
            front_mask[1:] = False
        
        # æå–poses
        front_poses = camera_poses[front_mask]
        frame_indices = data['frame_indices']
        front_frames = np.array(frame_indices)[front_mask]
        
        # æ’åº
        sorted_indices = np.argsort(front_frames)
        front_poses = front_poses[sorted_indices]
        
        print(f"   NPZ input: {len(front_poses)} poses -> Target: {target_frames} frames")
        
        # å…³é”®å¸§é€‰æ‹©
        actual_frames = len(front_poses)
        step = max(1, actual_frames // 4)
        key_poses = front_poses[::step]
        
        print(f"   Step size: {step} -> Key frames: {len(key_poses)}")
        
        # æ˜¾ç¤ºå…³é”®å¸§åæ ‡
        print(f"   Key frame positions:")
        for i, pose in enumerate(key_poses):
            pos = pose[:3, 3]
            print(f"     [{i}] {pos[0]:.3f}, {pos[1]:.3f}, {pos[2]:.3f}")
        
        # è½¬æ¢ä¸ºtensorå¹¶æ’å€¼
        key_poses_tensor = torch.tensor(key_poses, dtype=torch.float32)
        if per_cam_poses and len(per_cam_poses) > 0:
            sample_pose = per_cam_poses[list(per_cam_poses.keys())[0]]
            key_poses_tensor = key_poses_tensor.to(sample_pose.device)
        
        result = interpolate_poses(key_poses_tensor, target_frames)
        
        # è¾“å‡ºç»“æœ
        result_start = result[0][:3, 3]
        result_end = result[-1][:3, 3]
        print(f"   Output: {result.shape[0]} frames")
        print(f"   Start: {result_start[0]:.3f}, {result_start[1]:.3f}, {result_start[2]:.3f}")
        print(f"   End:   {result_end[0]:.3f}, {result_end[1]:.3f}, {result_end[2]:.3f}")
        
        return result
        
    except Exception as e:
        print(f"   Error: {e}")
        return analyze_front_center_interp(dataset_type, per_cam_poses, original_frames, target_frames, num_loops)

# FIX_TRAJ= "output/streetgs/dataset=Kitti/training_20250628_171319_FollowLeadingVehicle_1/camera_poses_eval/full_poses_2025-07-09_00-52-47.npz"
FIX_TRAJ= "output/pvg/dataset=Kitti/training_20250628_171319_FollowLeadingVehicle_1/camera_poses/full_poses_2025-07-08_21-34-38.npz"


def fixed_offset_trajectory_1(
    dataset_type: str,
    per_cam_poses: Dict[int, torch.Tensor],
    original_frames: int,
    target_frames: int,
) -> torch.Tensor:
    """åŸºäºkitti_fixed_path + [0.0, 0.0, 0.5]åç§»"""
    return kitti_fixed_path(
        dataset_type, 
        per_cam_poses, 
        original_frames, 
        target_frames,
        position_offset=[0.5, 0.0, 0.0],
        npz_path = FIX_TRAJ
    )

def fixed_offset_trajectory_2(
    dataset_type: str,
    per_cam_poses: Dict[int, torch.Tensor],
    original_frames: int,
    target_frames: int,
) -> torch.Tensor:
    """åŸºäºkitti_fixed_path + [3.2, 0.0, 0.0]åç§»"""
    return kitti_fixed_path(
        dataset_type, 
        per_cam_poses, 
        original_frames, 
        target_frames,
        position_offset=[0.0, -3.2, 0.0],
        npz_path = FIX_TRAJ
    )

def fixed_offset_trajectory_3(
    dataset_type: str,
    per_cam_poses: Dict[int, torch.Tensor],
    original_frames: int,
    target_frames: int,
) -> torch.Tensor:
    """åŸºäºkitti_fixed_path + [1.6, 0.0, 0.0]åç§»"""
    return kitti_fixed_path(
        dataset_type, 
        per_cam_poses, 
        original_frames, 
        target_frames,
        position_offset=[0.0, -1.6, 0.0],
        npz_path = FIX_TRAJ
    )

def fixed_offset_trajectory_4(
    dataset_type: str,
    per_cam_poses: Dict[int, torch.Tensor],
    original_frames: int,
    target_frames: int,
) -> torch.Tensor:
    """åŸºäºkitti_fixed_path + [-3.2, 0.0, 0.0]åç§»"""
    return kitti_fixed_path(
        dataset_type, 
        per_cam_poses, 
        original_frames, 
        target_frames,
        position_offset=[0.0, 3.2, 0.0],
        npz_path = FIX_TRAJ
    )

def fixed_offset_trajectory_5(
    dataset_type: str,
    per_cam_poses: Dict[int, torch.Tensor],
    original_frames: int,
    target_frames: int,
) -> torch.Tensor:
    """åŸºäºkitti_fixed_path + [-1.6, 0.0, 0.0]åç§»"""
    return kitti_fixed_path(
        dataset_type, 
        per_cam_poses, 
        original_frames, 
        target_frames,
        position_offset=[0.0, 1.6, 0.0],
        npz_path = FIX_TRAJ
    )

def fixed_offset_trajectory_6(
    dataset_type: str,
    per_cam_poses: Dict[int, torch.Tensor],
    original_frames: int,
    target_frames: int,
) -> torch.Tensor:
    """åŸºäºkitti_fixed_path + [0.5, 0.0, 0.0]åç§»"""
    return kitti_fixed_path(
        dataset_type, 
        per_cam_poses, 
        original_frames, 
        target_frames,
        position_offset=[0.0, -0.5, 0.0],
        npz_path = FIX_TRAJ
    )

def fixed_offset_trajectory_7(
    dataset_type: str,
    per_cam_poses: Dict[int, torch.Tensor],
    original_frames: int,
    target_frames: int,
) -> torch.Tensor:
    """åŸºäºkitti_fixed_path + [-0.5, 0.0, 0.0]åç§»"""
    return kitti_fixed_path(
        dataset_type, 
        per_cam_poses, 
        original_frames, 
        target_frames,
        position_offset=[0.0, 0.5, 0.0],
        npz_path = FIX_TRAJ
    )

def fixed_offset_trajectory_8(
    dataset_type: str,
    per_cam_poses: Dict[int, torch.Tensor],
    original_frames: int,
    target_frames: int,
) -> torch.Tensor:
    """åŸºäºkitti_fixed_path + [0.5, 0.0, 0.0]åç§» + Yè½´æ—‹è½¬15åº¦"""
    return kitti_fixed_path(
        dataset_type, 
        per_cam_poses, 
        original_frames, 
        target_frames,
        position_offset=[0.0, -0.5, 0.0],
        rotation_offset=[0.0, 0.0, math.radians(-15.0)],  # è½¬æ¢ä¸ºå¼§åº¦
        npz_path = FIX_TRAJ
    )

def fixed_offset_trajectory_9(
    dataset_type: str,
    per_cam_poses: Dict[int, torch.Tensor],
    original_frames: int,
    target_frames: int,
) -> torch.Tensor:
    """åŸºäºkitti_fixed_path + [-0.5, 0.0, 0.0]åç§» + Yè½´æ—‹è½¬-15åº¦"""
    return kitti_fixed_path(
        dataset_type, 
        per_cam_poses, 
        original_frames, 
        target_frames,
        position_offset=[0.0, 0.5, 0.0],
        rotation_offset=[0.0, 0.0, math.radians(15.0)],  # è½¬æ¢ä¸ºå¼§åº¦
        npz_path = FIX_TRAJ
    )

def fixed_offset_trajectory_10(
    dataset_type: str,
    per_cam_poses: Dict[int, torch.Tensor],
    original_frames: int,
    target_frames: int,
) -> torch.Tensor:
    """åŸºäºkitti_fixed_path + [0.0, 0.0, -0.5]åç§»"""
    return kitti_fixed_path(
        dataset_type, 
        per_cam_poses, 
        original_frames, 
        target_frames,
        position_offset=[-0.5, 0.0, 0.0],
        npz_path = FIX_TRAJ
    )

def double_lane_change_trajectory(
    dataset_type: str,
    per_cam_poses: Dict[int, torch.Tensor],
    original_frames: int,
    target_frames: int,
    first_change_start: int = 20,
    first_change_end: int = 50,
    second_change_start: int = 110,
    second_change_end: int = 130,
    lane_offset: float = 3.2,
    offset_vector: list = [0.0, -1.0, 0.0],  # å‘å·¦å˜é“
    return_offset_vector: list = [0.0, 1.0, 0.0],  # å‘å³è¿”å›
    first_steer_angle: float = 5.0,   # ç¬¬ä¸€æ¬¡å˜é“æ—¶çš„è½¬å‘è§’åº¦(åº¦)
    second_steer_angle: float = -5.0,  # ç¬¬äºŒæ¬¡å˜é“æ—¶çš„è½¬å‘è§’åº¦(åº¦)
) -> torch.Tensor:
    """
    ç”Ÿæˆå¸¦è½¬å‘çš„å˜é“-å˜å›è½¨è¿¹
    
    æ¯æ¬¡å˜é“éƒ½åŒ…å«ï¼šè½¬å‘->å›æ­£ çš„è¿‡ç¨‹
    è½¬å‘ä»change_startå¼€å§‹ï¼Œåœ¨change_endç»“æŸ
    è½¬å‘ç»•Yè½´æ—‹è½¬ï¼ˆæ°´å¹³è½¬å‘ï¼‰
    """
    import math
    import torch
    import numpy as np
    from scipy.spatial.transform import Rotation as R
    
    assert 0 in per_cam_poses.keys(), "éœ€è¦å‰è§†ä¸­å¿ƒç›¸æœºï¼ˆID 0ï¼‰"
    assert (
        0
        <= first_change_start
        < first_change_end
        < second_change_start
        < second_change_end
        < target_frames
    ), "å¸§ç´¢å¼•è®¾ç½®æœ‰è¯¯"
    
    # è·å–è®¾å¤‡ä¿¡æ¯
    device = per_cam_poses[0].device
    
    # ç”ŸæˆåŸºç¡€è½¨è¿¹ï¼ˆä½¿ç”¨front_center_interpï¼‰
    base_trajectory = front_center_interp(
        dataset_type, per_cam_poses, original_frames, target_frames
    )
    
    # å½’ä¸€åŒ–ç¬¬ä¸€æ¬¡å˜é“çš„åç§»å‘é‡
    first_vector = torch.tensor(offset_vector, device=device, dtype=torch.float32)
    first_vector = first_vector / torch.norm(first_vector)
    first_full_offset = first_vector * lane_offset
    
    # å½’ä¸€åŒ–ç¬¬äºŒæ¬¡å˜é“çš„åç§»å‘é‡
    second_vector = torch.tensor(
        return_offset_vector, device=device, dtype=torch.float32
    )
    second_vector = second_vector / torch.norm(second_vector)
    second_full_offset = second_vector * lane_offset
    
    # åˆ›å»ºå˜é“è½¨è¿¹
    lane_change_trajectory = base_trajectory.clone()
    
    # åˆ›å»ºè½¬å‘æ—‹è½¬ï¼ˆç»•Yè½´æ—‹è½¬ - æ°´å¹³è½¬å‘ï¼‰
    # ç¬¬ä¸€æ¬¡å˜é“çš„è½¬å‘æ—‹è½¬
    first_rot = R.from_euler('y', first_steer_angle, degrees=True)
    first_rot_matrix = torch.tensor(first_rot.as_matrix(), device=device, dtype=torch.float32)
    
    # ç¬¬äºŒæ¬¡å˜é“çš„è½¬å‘æ—‹è½¬
    second_rot = R.from_euler('y', second_steer_angle, degrees=True)
    second_rot_matrix = torch.tensor(second_rot.as_matrix(), device=device, dtype=torch.float32)
    
    # å•ä½æ—‹è½¬çŸ©é˜µï¼ˆç›´è¡ŒçŠ¶æ€ï¼‰
    identity_rot = torch.eye(3, device=device, dtype=torch.float32)
    
    # è‡ªå®šä¹‰çƒé¢çº¿æ€§æ’å€¼å‡½æ•°
    def custom_slerp(rot1, rot2, t):
        """
        è‡ªå®šä¹‰çš„çƒé¢çº¿æ€§æ’å€¼ï¼Œå…¼å®¹æ—§ç‰ˆæœ¬scipy
        """
        q1 = rot1.as_quat()
        q2 = rot2.as_quat()
        
        # è®¡ç®—å››å…ƒæ•°ç‚¹ç§¯
        dot = np.dot(q1, q2)
        
        # å¦‚æœç‚¹ç§¯ä¸ºè´Ÿï¼Œåè½¬ç¬¬äºŒä¸ªå››å…ƒæ•°ä»¥ç¡®ä¿æœ€çŸ­è·¯å¾„
        if dot < 0.0:
            q2 = -q2
            dot = -dot
        
        # å¦‚æœå››å…ƒæ•°å‡ ä¹ç›¸åŒï¼Œç›´æ¥çº¿æ€§æ’å€¼
        if dot > 0.9995:
            result = q1 + t * (q2 - q1)
            result /= np.linalg.norm(result)
            return R.from_quat(result)
        
        # è®¡ç®—æ’å€¼è§’åº¦
        theta_0 = np.arccos(np.abs(dot))
        sin_theta_0 = np.sin(theta_0)
        
        theta = theta_0 * t
        sin_theta = np.sin(theta)
        
        s0 = np.cos(theta) - dot * sin_theta / sin_theta_0
        s1 = sin_theta / sin_theta_0
        
        # çƒé¢çº¿æ€§æ’å€¼
        result = s0 * q1 + s1 * q2
        return R.from_quat(result)
    
    # è®¡ç®—æ¯æ¬¡å˜é“çš„é˜¶æ®µé•¿åº¦
    first_duration = first_change_end - first_change_start
    first_steer_duration = first_duration // 2  # è½¬å‘é˜¶æ®µï¼ˆå‰åŠæ®µï¼‰
    first_return_duration = first_duration - first_steer_duration  # å›æ­£é˜¶æ®µï¼ˆååŠæ®µï¼‰
    
    second_duration = second_change_end - second_change_start
    second_steer_duration = second_duration // 2  # è½¬å‘é˜¶æ®µï¼ˆå‰åŠæ®µï¼‰
    second_return_duration = second_duration - second_steer_duration  # å›æ­£é˜¶æ®µï¼ˆååŠæ®µï¼‰
    
    # åº”ç”¨å˜é“å’Œè½¬å‘
    for frame_idx in range(target_frames):
        original_rotation = base_trajectory[frame_idx, :3, :3].clone()
        
        if frame_idx < first_change_start:
            # ç¬¬ä¸€æ¬¡å˜é“ä¹‹å‰ä¿æŒåŸå§‹è½¨è¿¹
            continue
            
        elif frame_idx < first_change_start + first_steer_duration:
            # ç¬¬ä¸€æ¬¡å˜é“ï¼šè½¬å‘é˜¶æ®µï¼ˆå‰åŠæ®µï¼‰
            progress = (frame_idx - first_change_start) / first_steer_duration
            smooth_factor = 0.5 - 0.5 * math.cos(math.pi * progress)
            
            # å¹³æ»‘ä½ç§»
            current_offset = first_full_offset * ((frame_idx - first_change_start) / first_duration)
            lane_change_trajectory[frame_idx, :3, 3] += current_offset
            
            # å¹³æ»‘è½¬å‘
            start_rot = R.from_matrix(identity_rot.cpu().numpy())
            end_rot = R.from_matrix(first_rot_matrix.cpu().numpy())
            interpolated_rot = custom_slerp(start_rot, end_rot, smooth_factor)
            interpolated_rot_matrix = torch.tensor(
                interpolated_rot.as_matrix(), 
                device=device, 
                dtype=torch.float32
            )
            
            # åº”ç”¨å¹³æ»‘æ—‹è½¬
            lane_change_trajectory[frame_idx, :3, :3] = original_rotation @ interpolated_rot_matrix
            
        elif frame_idx <= first_change_end:
            # ç¬¬ä¸€æ¬¡å˜é“ï¼šå›æ­£é˜¶æ®µï¼ˆååŠæ®µï¼‰
            progress = (frame_idx - first_change_start - first_steer_duration) / first_return_duration
            smooth_factor = 0.5 - 0.5 * math.cos(math.pi * progress)
            
            # ç»§ç»­ç§»åŠ¨
            current_offset = first_full_offset * ((frame_idx - first_change_start) / first_duration)
            lane_change_trajectory[frame_idx, :3, 3] += current_offset
            
            # ä»è½¬å‘å›åˆ°ç›´è¡Œ
            start_rot = R.from_matrix(first_rot_matrix.cpu().numpy())
            end_rot = R.from_matrix(identity_rot.cpu().numpy())
            interpolated_rot = custom_slerp(start_rot, end_rot, smooth_factor)
            interpolated_rot_matrix = torch.tensor(
                interpolated_rot.as_matrix(), 
                device=device, 
                dtype=torch.float32
            )
            
            # åº”ç”¨å¹³æ»‘æ—‹è½¬
            lane_change_trajectory[frame_idx, :3, :3] = original_rotation @ interpolated_rot_matrix
            
        elif frame_idx < second_change_start:
            # åœ¨ä¸¤æ¬¡å˜é“ä¹‹é—´ä¿æŒç›´è¡Œ
            lane_change_trajectory[frame_idx, :3, 3] += first_full_offset
            # ä¿æŒç›´è¡ŒçŠ¶æ€
            lane_change_trajectory[frame_idx, :3, :3] = original_rotation @ identity_rot
            
        elif frame_idx < second_change_start + second_steer_duration:
            # ç¬¬äºŒæ¬¡å˜é“ï¼šè½¬å‘é˜¶æ®µï¼ˆå‰åŠæ®µï¼‰
            progress = (frame_idx - second_change_start) / second_steer_duration
            smooth_factor = 0.5 - 0.5 * math.cos(math.pi * progress)
            
            # å¹³æ»‘è¿”å›
            total_progress = (frame_idx - second_change_start) / second_duration
            current_offset = first_full_offset + second_full_offset * total_progress
            lane_change_trajectory[frame_idx, :3, 3] += current_offset
            
            # å¹³æ»‘è½¬å‘
            start_rot = R.from_matrix(identity_rot.cpu().numpy())
            end_rot = R.from_matrix(second_rot_matrix.cpu().numpy())
            interpolated_rot = custom_slerp(start_rot, end_rot, smooth_factor)
            interpolated_rot_matrix = torch.tensor(
                interpolated_rot.as_matrix(), 
                device=device, 
                dtype=torch.float32
            )
            
            # åº”ç”¨å¹³æ»‘æ—‹è½¬
            lane_change_trajectory[frame_idx, :3, :3] = original_rotation @ interpolated_rot_matrix
            
        elif frame_idx <= second_change_end:
            # ç¬¬äºŒæ¬¡å˜é“ï¼šå›æ­£é˜¶æ®µï¼ˆååŠæ®µï¼‰
            progress = (frame_idx - second_change_start - second_steer_duration) / second_return_duration
            smooth_factor = 0.5 - 0.5 * math.cos(math.pi * progress)
            
            # ç»§ç»­è¿”å›
            total_progress = (frame_idx - second_change_start) / second_duration
            current_offset = first_full_offset + second_full_offset * total_progress
            lane_change_trajectory[frame_idx, :3, 3] += current_offset
            
            # ä»è½¬å‘å›åˆ°ç›´è¡Œ
            start_rot = R.from_matrix(second_rot_matrix.cpu().numpy())
            end_rot = R.from_matrix(identity_rot.cpu().numpy())
            interpolated_rot = custom_slerp(start_rot, end_rot, smooth_factor)
            interpolated_rot_matrix = torch.tensor(
                interpolated_rot.as_matrix(), 
                device=device, 
                dtype=torch.float32
            )
            
            # åº”ç”¨å¹³æ»‘æ—‹è½¬
            lane_change_trajectory[frame_idx, :3, :3] = original_rotation @ interpolated_rot_matrix
            
        else:
            # ç¬¬äºŒæ¬¡å˜é“ä¹‹åä¿æŒç›´è¡ŒçŠ¶æ€
            # ä¸æ·»åŠ ä»»ä½•åç§»ï¼Œä¿æŒç›´è¡Œ
            continue
    
    return lane_change_trajectory


def smooth_lane_change_trajectory(
    dataset_type: str,
    per_cam_poses: Dict[int, torch.Tensor],
    original_frames: int,
    target_frames: int,
    start_frame: int = 20,
    end_frame: int = 50,
    lane_offset: float = -3.2,
    offset_vector: list = [0.0, 1.0, 0.0],  # å‘å·¦å˜é“
    steer_angle: float = -5.0,  # è½¬å‘è§’åº¦(åº¦)
) -> torch.Tensor:
    """
    ç”Ÿæˆå¸¦è½¬å‘çš„å•æ¬¡å˜é“è½¨è¿¹
    
    å˜é“è¿‡ç¨‹åŒ…å«ï¼šè½¬å‘->å›æ­£ çš„è¿‡ç¨‹
    è½¬å‘ä»start_frameå¼€å§‹ï¼Œåœ¨end_frameç»“æŸ
    è½¬å‘ç»•Yè½´æ—‹è½¬ï¼ˆæ°´å¹³è½¬å‘ï¼‰
    """
    import math
    import torch
    import numpy as np
    from scipy.spatial.transform import Rotation as R
    
    assert 0 in per_cam_poses.keys(), "éœ€è¦å‰è§†ä¸­å¿ƒç›¸æœºï¼ˆID 0ï¼‰"
    assert 0 <= start_frame < end_frame < target_frames, "å¸§ç´¢å¼•è®¾ç½®æœ‰è¯¯"
    
    # è·å–è®¾å¤‡ä¿¡æ¯
    device = per_cam_poses[0].device
    
    # ç”ŸæˆåŸºç¡€è½¨è¿¹ï¼ˆä½¿ç”¨front_center_interpï¼‰
    base_trajectory = front_center_interp(
        dataset_type, per_cam_poses, original_frames, target_frames
    )
    
    # å½’ä¸€åŒ–åç§»å‘é‡
    norm_vector = torch.tensor(offset_vector, device=device, dtype=torch.float32)
    norm_vector = norm_vector / torch.norm(norm_vector)
    
    # è®¡ç®—å®Œæ•´åç§»é‡
    full_offset = norm_vector * lane_offset
    
    # åˆ›å»ºå˜é“è½¨è¿¹
    lane_change_trajectory = base_trajectory.clone()
    
    # åˆ›å»ºè½¬å‘æ—‹è½¬ï¼ˆç»•Yè½´æ—‹è½¬ - æ°´å¹³è½¬å‘ï¼‰
    steer_rot = R.from_euler('y', steer_angle, degrees=True)
    steer_rot_matrix = torch.tensor(steer_rot.as_matrix(), device=device, dtype=torch.float32)
    
    # å•ä½æ—‹è½¬çŸ©é˜µï¼ˆç›´è¡ŒçŠ¶æ€ï¼‰
    identity_rot = torch.eye(3, device=device, dtype=torch.float32)
    
    # è‡ªå®šä¹‰çƒé¢çº¿æ€§æ’å€¼å‡½æ•°
    def custom_slerp(rot1, rot2, t):
        """
        è‡ªå®šä¹‰çš„çƒé¢çº¿æ€§æ’å€¼ï¼Œå…¼å®¹æ—§ç‰ˆæœ¬scipy
        """
        q1 = rot1.as_quat()
        q2 = rot2.as_quat()
        
        # è®¡ç®—å››å…ƒæ•°ç‚¹ç§¯
        dot = np.dot(q1, q2)
        
        # å¦‚æœç‚¹ç§¯ä¸ºè´Ÿï¼Œåè½¬ç¬¬äºŒä¸ªå››å…ƒæ•°ä»¥ç¡®ä¿æœ€çŸ­è·¯å¾„
        if dot < 0.0:
            q2 = -q2
            dot = -dot
        
        # å¦‚æœå››å…ƒæ•°å‡ ä¹ç›¸åŒï¼Œç›´æ¥çº¿æ€§æ’å€¼
        if dot > 0.9995:
            result = q1 + t * (q2 - q1)
            result /= np.linalg.norm(result)
            return R.from_quat(result)
        
        # è®¡ç®—æ’å€¼è§’åº¦
        theta_0 = np.arccos(np.abs(dot))
        sin_theta_0 = np.sin(theta_0)
        
        theta = theta_0 * t
        sin_theta = np.sin(theta)
        
        s0 = np.cos(theta) - dot * sin_theta / sin_theta_0
        s1 = sin_theta / sin_theta_0
        
        # çƒé¢çº¿æ€§æ’å€¼
        result = s0 * q1 + s1 * q2
        return R.from_quat(result)
    
    # è®¡ç®—å˜é“çš„é˜¶æ®µé•¿åº¦
    duration = end_frame - start_frame
    steer_duration = duration // 2  # è½¬å‘é˜¶æ®µï¼ˆå‰åŠæ®µï¼‰
    return_duration = duration - steer_duration  # å›æ­£é˜¶æ®µï¼ˆååŠæ®µï¼‰
    
    # å¯¹æ¯ä¸€å¸§åº”ç”¨å¹³æ»‘è¿‡æ¸¡çš„åç§»å’Œè½¬å‘
    for frame_idx in range(target_frames):
        original_rotation = base_trajectory[frame_idx, :3, :3].clone()
        
        if frame_idx < start_frame:
            # èµ·å§‹å¸§ä¹‹å‰ä¿æŒåŸå§‹è½¨è¿¹
            continue
            
        elif frame_idx < start_frame + steer_duration:
            # è½¬å‘é˜¶æ®µï¼ˆå‰åŠæ®µï¼‰
            progress = (frame_idx - start_frame) / steer_duration
            smooth_factor = 0.5 - 0.5 * math.cos(math.pi * progress)
            
            # å¹³æ»‘ä½ç§»
            current_offset = full_offset * ((frame_idx - start_frame) / duration)
            lane_change_trajectory[frame_idx, :3, 3] += current_offset
            
            # å¹³æ»‘è½¬å‘
            start_rot = R.from_matrix(identity_rot.cpu().numpy())
            end_rot = R.from_matrix(steer_rot_matrix.cpu().numpy())
            interpolated_rot = custom_slerp(start_rot, end_rot, smooth_factor)
            interpolated_rot_matrix = torch.tensor(
                interpolated_rot.as_matrix(), 
                device=device, 
                dtype=torch.float32
            )
            
            # åº”ç”¨å¹³æ»‘æ—‹è½¬
            lane_change_trajectory[frame_idx, :3, :3] = original_rotation @ interpolated_rot_matrix
            
        elif frame_idx <= end_frame:
            # å›æ­£é˜¶æ®µï¼ˆååŠæ®µï¼‰
            progress = (frame_idx - start_frame - steer_duration) / return_duration
            smooth_factor = 0.5 - 0.5 * math.cos(math.pi * progress)
            
            # ç»§ç»­ç§»åŠ¨
            current_offset = full_offset * ((frame_idx - start_frame) / duration)
            lane_change_trajectory[frame_idx, :3, 3] += current_offset
            
            # ä»è½¬å‘å›åˆ°ç›´è¡Œ
            start_rot = R.from_matrix(steer_rot_matrix.cpu().numpy())
            end_rot = R.from_matrix(identity_rot.cpu().numpy())
            interpolated_rot = custom_slerp(start_rot, end_rot, smooth_factor)
            interpolated_rot_matrix = torch.tensor(
                interpolated_rot.as_matrix(), 
                device=device, 
                dtype=torch.float32
            )
            
            # åº”ç”¨å¹³æ»‘æ—‹è½¬
            lane_change_trajectory[frame_idx, :3, :3] = original_rotation @ interpolated_rot_matrix
        else:
            # ç»“æŸå¸§ä¹‹åä¿æŒæ–°ä½ç½®å’Œç›´è¡ŒçŠ¶æ€
            lane_change_trajectory[frame_idx, :3, 3] += full_offset
            # ä¿æŒç›´è¡ŒçŠ¶æ€
            lane_change_trajectory[frame_idx, :3, :3] = original_rotation @ identity_rot
    
    return lane_change_trajectory

def s_curve(
    dataset_type: str,
    per_cam_poses: Dict[int, torch.Tensor],
    original_frames: int,
    target_frames: int,
) -> torch.Tensor:
    """Create an S-shaped trajectory using the front three cameras."""
    assert all(
        cam in per_cam_poses.keys() for cam in [0, 1, 2]
    ), "Front three cameras (IDs 0, 1, 2) are required for s_curve"
    key_poses = torch.cat(
        [
            per_cam_poses[0][0:1],
            per_cam_poses[1][original_frames // 4 : original_frames // 4 + 1],
            per_cam_poses[0][original_frames // 2 : original_frames // 2 + 1],
            per_cam_poses[2][3 * original_frames // 4 : 3 * original_frames // 4 + 1],
            per_cam_poses[0][-1:],
        ],
        dim=0,
    )
    return interpolate_poses(key_poses, target_frames)


def three_key_poses_trajectory(
    dataset_type: str,
    per_cam_poses: Dict[int, torch.Tensor],
    original_frames: int,
    target_frames: int,
) -> torch.Tensor:
    """
    Create a trajectory using three key poses:
    1. First frame of front center camera
    2. Middle frame with interpolated rotation and position from camera 1 or 2
    3. Last frame of front center camera

    The rotation of the middle pose is calculated using Slerp between
    the start frame and the middle frame of camera 1 or 2.

    Args:
        dataset_type (str): Type of the dataset (e.g., "waymo", "pandaset", etc.).
        per_cam_poses (Dict[int, torch.Tensor]): Dictionary of camera poses.
        original_frames (int): Number of original frames.
        target_frames (int): Number of frames in the output trajectory.

    Returns:
        torch.Tensor: Trajectory of shape (target_frames, 4, 4).
    """
    assert 0 in per_cam_poses.keys(), "Front center camera (ID 0) is required"
    assert (
        1 in per_cam_poses.keys() or 2 in per_cam_poses.keys()
    ), "Either camera 1 or camera 2 is required"

    # First key pose: First frame of front center camera
    start_pose = per_cam_poses[0][0]
    key_poses = [start_pose]

    # Select camera for middle frame
    middle_frame = int(original_frames // 2)
    chosen_cam = np.random.choice([1, 2])

    middle_pose = per_cam_poses[chosen_cam][middle_frame]

    # Calculate interpolated rotation for middle pose
    start_rotation = R.from_matrix(start_pose[:3, :3].cpu().numpy())
    middle_rotation = R.from_matrix(middle_pose[:3, :3].cpu().numpy())
    slerp = Slerp(
        [0, 1], R.from_quat([start_rotation.as_quat(), middle_rotation.as_quat()])
    )
    interpolated_rotation = slerp(0.5).as_matrix()

    # Create middle key pose with interpolated rotation and original translation
    middle_key_pose = torch.eye(4, device=start_pose.device)
    middle_key_pose[:3, :3] = torch.tensor(
        interpolated_rotation, device=start_pose.device
    )
    middle_key_pose[:3, 3] = middle_pose[:3, 3]  # Keep the original translation
    key_poses.append(middle_key_pose)

    # Third key pose: Last frame of front center camera
    key_poses.append(per_cam_poses[0][-1])

    # Stack the key poses and interpolate
    key_poses = torch.stack(key_poses)
    return interpolate_poses(key_poses, target_frames)


def circle_trajectory(
    dataset_type: str,
    per_cam_poses: Dict[int, torch.Tensor],
    original_frames: int,
    target_frames: int,
    radius: float = 5.0,
    height: float = 2.0,
) -> torch.Tensor:
    """ç”Ÿæˆç¯ç»•åœºæ™¯çš„åœ†å½¢è½¨è¿¹"""
    # ä¿®å¤1ï¼šæ­£ç¡®è·å–ä¸­å¿ƒç‚¹åæ ‡
    center_pose = per_cam_poses[0][original_frames // 2]
    center = center_pose[:3, 3].cpu().numpy()  # [!code --]
    center = center_pose[:3, 3].cpu().numpy()  # [!code ++] ç›´æ¥å–ä½ç½®åæ ‡

    # ä¿®å¤2ï¼šæ·»åŠ è°ƒè¯•ä¿¡æ¯
    print(f"Center pose shape: {center_pose.shape}")  # åº”ä¸º (4,4)
    print(f"Center coordinates: {center}")  # åº”æ˜¾ç¤ºä¸‰ç»´åæ ‡

    # ç”Ÿæˆåœ†å½¢è½¨è¿¹å‚æ•°
    angles = np.linspace(0, 2 * np.pi, 12)
    key_poses = []

    for angle in angles:
        x = center[0] + radius * np.cos(angle)
        y = center[1] + radius * np.sin(angle)
        z = center[2] + height  # [!code ++]

        # ç¡®ä¿åæ ‡ç±»å‹æ­£ç¡®
        pose = torch.eye(4, device=center_pose.device)
        pose[:3, 3] = torch.tensor([x, y, z], device=center_pose.device)

        # ä¿®å¤3ï¼šæ·»åŠ æ–¹å‘è®¡ç®—ä¿æŠ¤
        direction = center - pose[:3, 3].cpu().numpy()
        if np.linalg.norm(direction) < 1e-6:
            direction = np.array([0.0, 0.0, 1.0])  # é˜²æ­¢é›¶å‘é‡

        pose[:3, :3] = look_at_rotation(
            torch.tensor(direction, device=center_pose.device)
        )
        key_poses.append(pose)

    return interpolate_poses(torch.stack(key_poses), target_frames)


def spiral_trajectory(
    dataset_type: str,
    per_cam_poses: Dict[int, torch.Tensor],
    original_frames: int,
    target_frames: int,
    radius: float = 5.0,
    spiral_height: float = 3.0,
    num_turns: int = 2,
) -> torch.Tensor:
    """ç”Ÿæˆèºæ—‹ä¸Šå‡è½¨è¿¹"""
    center_pose = per_cam_poses[0][original_frames // 2]
    center = center_pose[:3, 3].mean(dim=0).cpu().numpy()

    angles = np.linspace(0, num_turns * 2 * np.pi, 12)
    key_poses = []
    for i, angle in enumerate(angles):
        r = radius * (1 - i / len(angles))  # åŠå¾„é€æ¸ç¼©å°
        x = center[0] + r * np.cos(angle)
        y = center[1] + r * np.sin(angle)
        z = center[2] + spiral_height * (i / len(angles))

        pose = torch.eye(4, device=center_pose.device)
        pose[:3, 3] = torch.tensor([x, y, z], device=center_pose.device)
        direction = center - pose[:3, 3].cpu().numpy()
        pose[:3, :3] = look_at_rotation(
            torch.tensor(direction, device=center_pose.device)
        )

        key_poses.append(pose)

    return interpolate_poses(torch.stack(key_poses), target_frames)


def look_around_trajectory(
    dataset_type: str,
    per_cam_poses: Dict[int, torch.Tensor],
    original_frames: int,
    target_frames: int,
    elevation_range: tuple = (-30, 30),
    azimuth_range: tuple = (0, 360),
) -> torch.Tensor:
    """ç”Ÿæˆç¯ç»•è§‚å¯Ÿè½¨è¿¹ï¼ˆå›ºå®šä½ç½®ï¼Œæ—‹è½¬è§†è§’ï¼‰"""
    center_pose = per_cam_poses[0][original_frames // 2]
    center = center_pose[:3, 3].cpu().numpy()

    # ç”Ÿæˆè§†è§’å‚æ•°
    elevations = np.linspace(*elevation_range, 6)
    azimuths = np.linspace(*azimuth_range, 6)

    key_poses = []
    for elev, azim in zip(elevations, azimuths):
        # å°†çƒåæ ‡è½¬æ¢ä¸ºç¬›å¡å°”åæ ‡
        r = np.linalg.norm(center)
        x = r * np.cos(np.radians(azim)) * np.cos(np.radians(elev))
        y = r * np.sin(np.radians(azim)) * np.cos(np.radians(elev))
        z = r * np.sin(np.radians(elev))

        pose = torch.eye(4, device=center_pose.device)
        pose[:3, 3] = torch.tensor([x, y, z], device=center_pose.device)
        direction = center - pose[:3, 3].cpu().numpy()
        pose[:3, :3] = look_at_rotation(
            torch.tensor(direction, device=center_pose.device)
        )

        key_poses.append(pose)

    return interpolate_poses(torch.stack(key_poses), target_frames)